# -*- coding: utf-8 -*-
"""pcmb_classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pslIVYOmVOnWYDjvYMAgKVCIgpr5pGNg
"""
import pickle

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Conv1D, MaxPooling1D
from tensorflow.keras.layers import LSTM, Embedding, GRU
import tensorflow as tf

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

def plot(history, info_type='loss'):

    """
    history: the history callback from a model.fit
    info_type: what you want to show. (e.g. 'loss', 'acc', 'accuracy')
    """
    plt.plot(history.history[info_type], label=[info_type])
    try:
        plt.plot(history.history['val_' + info_type], label=['val_' + info_type])
    except Exception:
        print(f'no val_{info_type}')
    plt.title(info_type)
    plt.legend()

df = pd.read_csv('subjects-questions.csv')

#df.head()

df['ca_labels'] = df['Subject'].map({'Biology':0, 'Chemistry':1, 'Maths':2, 'Physics':3})

y = df['ca_labels'].values

#df['ca_labels'].values

#y.shape

x = df['eng'].values

for i in range(len(x)):
    x[i] = x[i].replace('\n', ' ')

x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.33, shuffle=True)

vocab_size = 1000
oov_token = '<oov>'
trunc='post'
maxlen = 50
embedded_dim = 8

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)

tokenizer.fit_on_texts(x_train)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(x_train)

#len(word_index)

padded = pad_sequences(sequences, maxlen=maxlen, truncating=trunc)

#padded.shape

testing_sequence = tokenizer.texts_to_sequences(x_test)
test_padded = pad_sequences(testing_sequence, maxlen=maxlen)

#test_padded.shape

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size,embedded_dim,input_length=maxlen),
    tf.keras.layers.Conv1D(64, 3, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(50, activation='relu', activity_regularizer=tf.keras.regularizers.L2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(50, activation='relu', activity_regularizer=tf.keras.regularizers.L2(0.01)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(4, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy',metrics=['acc'], optimizer='adam')

hist = model.fit(padded,y_train,batch_size=64, epochs=20, validation_data=(test_padded,y_test), validation_batch_size=64)

with open('tokenizer20.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

model.save('pcmb20_model.h5')